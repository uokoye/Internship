{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff77370",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2b17f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974cfc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search form not found.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Fill in search form and click the search button\n",
    "search_form = soup.find(\"form\", id=\"home_search\")\n",
    "if search_form:\n",
    "    job_title_input = search_form.find(\"input\", id=\"search-keyword\")\n",
    "    location_input = search_form.find(\"input\", id=\"search-location\")\n",
    "    search_button = search_form.find(\"button\", class_=\"search-btn\")\n",
    "\n",
    "    job_title_input[\"value\"] = \"Data Analyst\"\n",
    "    location_input[\"value\"] = \"Bangalore\"\n",
    "    response_after_search = requests.post(url, data=search_form.form_data)\n",
    "\n",
    "    # Step 4: Scrape the data for the first 10 jobs\n",
    "    job_results = response_after_search.find_all(\"div\", class_=\"result-display__profile\")\n",
    "\n",
    "    scraped_data = []\n",
    "    for job_result in job_results[:10]:\n",
    "        job_title = job_result.find(\"h3\", class_=\"job_title\").text.strip()\n",
    "        job_location = job_result.find(\"span\", class_=\"job_location\").text.strip()\n",
    "        company_name = job_result.find(\"span\", class_=\"company_name\").text.strip()\n",
    "        experience_required = job_result.find(\"li\", class_=\"d-flex\").find_all(\"span\")[1].text.strip()\n",
    "\n",
    "        scraped_data.append({\n",
    "            \"Job Title\": job_title,\n",
    "            \"Job Location\": job_location,\n",
    "            \"Company Name\": company_name,\n",
    "            \"Experience Required\": experience_required\n",
    "        })\n",
    "\n",
    "    # Step 5: Create a DataFrame\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Search form not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b5c8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search form not found.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Fill in search form and click the search button\n",
    "search_form = soup.find(\"form\", id=\"home_search\")\n",
    "if search_form:\n",
    "    job_title_input = search_form.find(\"input\", id=\"search-keyword\")\n",
    "    location_input = search_form.find(\"input\", id=\"search-location\")\n",
    "    search_button = search_form.find(\"button\", class_=\"search-btn\")\n",
    "\n",
    "    job_title_input[\"value\"] = \"Data Scientist\"\n",
    "    location_input[\"value\"] = \"Bangalore\"\n",
    "    response_after_search = requests.post(url, data=search_form.form_data)\n",
    "\n",
    "    # Step 4: Scrape the data for the first 10 jobs\n",
    "    job_results = response_after_search.find_all(\"div\", class_=\"result-display__profile\")\n",
    "\n",
    "    scraped_data = []\n",
    "    for job_result in job_results[:10]:\n",
    "        job_title = job_result.find(\"h3\", class_=\"job_title\").text.strip()\n",
    "        job_location = job_result.find(\"span\", class_=\"job_location\").text.strip()\n",
    "        company_name = job_result.find(\"span\", class_=\"company_name\").text.strip()\n",
    "\n",
    "        scraped_data.append({\n",
    "            \"Job Title\": job_title,\n",
    "            \"Job Location\": job_location,\n",
    "            \"Company Name\": company_name\n",
    "        })\n",
    "\n",
    "    # Step 5: Create a DataFrame\n",
    "    df = pd.DataFrame(scraped_data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "else:\n",
    "    print(\"Search form not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "In this question you have to scrape data using the filters available on the webpage\n",
    " You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scrapeddata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e7c71d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search form not found.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the webpage\n",
    "url = \"https://www.shine.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Fill in search form and click the search button\n",
    "search_form = soup.find(\"form\", id=\"home_search\")\n",
    "if search_form:\n",
    "    job_title_input = search_form.find(\"input\", id=\"search-keyword\")\n",
    "    search_button = search_form.find(\"button\", class_=\"search-btn\")\n",
    "\n",
    "    job_title_input[\"value\"] = \"Data Scientist\"\n",
    "    response_after_search = requests.post(url, data=search_form.form_data)\n",
    "\n",
    "    # Step 3: Apply filters and click the filter button\n",
    "    filter_form = response_after_search.find(\"form\", id=\"jobfilter_form\")\n",
    "    if filter_form:\n",
    "        location_checkbox = filter_form.find(\"input\", id=\"chk_loc_16\")\n",
    "        salary_checkbox = filter_form.find(\"input\", id=\"chk_sal_9\")\n",
    "        filter_button = filter_form.find(\"button\", class_=\"btn btn-search\")\n",
    "\n",
    "        location_checkbox[\"checked\"] = \"checked\"\n",
    "        salary_checkbox[\"checked\"] = \"checked\"\n",
    "\n",
    "        response_after_filters = requests.post(url, data=filter_form.form_data)\n",
    "\n",
    "        # Step 5: Scrape the data for the first 10 jobs\n",
    "        job_results = response_after_filters.find_all(\"div\", class_=\"result-display__profile\")\n",
    "\n",
    "        scraped_data = []\n",
    "        for job_result in job_results[:10]:\n",
    "            job_title = job_result.find(\"h3\", class_=\"job_title\").text.strip()\n",
    "            job_location = job_result.find(\"span\", class_=\"job_location\").text.strip()\n",
    "            company_name = job_result.find(\"span\", class_=\"company_name\").text.strip()\n",
    "            experience_required = job_result.find(\"li\", class_=\"d-flex\").find_all(\"span\")[1].text.strip()\n",
    "\n",
    "            scraped_data.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Job Location\": job_location,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Experience Required\": experience_required\n",
    "            })\n",
    "\n",
    "        # Step 6: Create a DataFrame\n",
    "        df = pd.DataFrame(scraped_data)\n",
    "\n",
    "        # Print the DataFrame\n",
    "        print(df)\n",
    "\n",
    "    else:\n",
    "        print(\"Filter form not found.\")\n",
    "\n",
    "else:\n",
    "    print(\"Search form not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "6. Brand\n",
    "7. ProductDescription\n",
    "8. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f0f7a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5644\\2626834963.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Loop through the product listings and scrape required attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mproduct\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct_listings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mbrand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"_2WkVRV\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mproduct_desc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"IRpwTa\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"_30jeq3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# URL of the Flipkart sunglasses listings\n",
    "url = \"https://www.flipkart.com/search?q=sunglasses\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the product listings\n",
    "product_listings = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through the product listings and scrape required attributes\n",
    "for product in product_listings[:100]:\n",
    "    brand = product.find(\"div\", class_=\"_2WkVRV\").text\n",
    "    product_desc = product.find(\"a\", class_=\"IRpwTa\").text\n",
    "    price = product.find(\"div\", class_=\"_30jeq3\").text\n",
    "\n",
    "    scraped_data.append({\n",
    "        \"Brand\": brand,\n",
    "        \"Product Description\": product_desc,\n",
    "        \"Price\": price\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e41e628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# URL of the Flipkart product reviews page\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the review listings\n",
    "review_listings = soup.find_all(\"div\", class_=\"_1PBCrt\")\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through the review listings and scrape the review text\n",
    "for review in review_listings[:100]:\n",
    "    review_text = review.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "    scraped_data.append({\"Review\": review_text})\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b9291",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3ac81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# URL of the Flipkart product reviews page\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the review listings\n",
    "review_listings = soup.find_all(\"div\", class_=\"_1PBCrt\")\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through the review listings and scrape the review text\n",
    "for review in review_listings[:100]:\n",
    "    review_text = review.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "    scraped_data.append({\"Review\": review_text})\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5feee92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Flipkart product reviews page\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the review listings\n",
    "review_listings = soup.find_all(\"div\", class_=\"_1PBCrt\")\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "# Loop through the review listings and scrape the review text\n",
    "for review in review_listings[:100]:\n",
    "    review_text = review.find(\"div\", class_=\"t-ZTKy\").text.strip()\n",
    "    scraped_data.append({\"Review\": review_text})\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4fc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes. 3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1af5ac40",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5644\\1920909404.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mquote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquote_listings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauthor_listings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mquote_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_listings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the AzQuotes top quotes page\n",
    "url = \"https://www.azquotes.com/\"\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Find and click on \"Top Quotes\"\n",
    "top_quotes_link = soup.find(\"a\", href=\"/top-quotes\")\n",
    "if top_quotes_link:\n",
    "    top_quotes_url = url + top_quotes_link[\"href\"]\n",
    "    response = requests.get(top_quotes_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 3: Scrape the data for top 1000 quotes\n",
    "quote_listings = soup.find_all(\"div\", class_=\"title\")\n",
    "author_listings = soup.find_all(\"div\", class_=\"author\")\n",
    "type_listings = soup.find_all(\"div\", class_=\"kw-box\")\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "for i in range(1000):\n",
    "    quote = quote_listings[i].text.strip()\n",
    "    author = author_listings[i].text.strip()\n",
    "    quote_type = type_listings[i].text.strip()\n",
    "    \n",
    "    scraped_data.append({\n",
    "        \"Quote\": quote,\n",
    "        \"Author\": author,\n",
    "        \"Type of Quote\": quote_type\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5131a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3eb6ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5644\\3355458593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Step 4: Scrape the data for former Prime Ministers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mpm_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"article-tab\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpm_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Skip the header row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mscraped_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL of the Jagran Josh webpage\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Find and click on \"GK\" option\n",
    "gk_option = soup.find(\"a\", text=\"GK\")\n",
    "if gk_option:\n",
    "    gk_url = url + gk_option[\"href\"]\n",
    "    response = requests.get(gk_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 3: Find and click on \"List of all Prime Ministers of India\"\n",
    "pm_list_link = soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n",
    "if pm_list_link:\n",
    "    pm_list_url = url + pm_list_link[\"href\"]\n",
    "    response = requests.get(pm_list_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 4: Scrape the data for former Prime Ministers\n",
    "pm_table = soup.find(\"table\", class_=\"article-tab\")\n",
    "rows = pm_table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "for row in rows:\n",
    "    columns = row.find_all(\"td\")\n",
    "    name = columns[0].text.strip()\n",
    "    born_dead = columns[1].text.strip()\n",
    "    term_of_office = columns[2].text.strip()\n",
    "    remarks = columns[3].text.strip()\n",
    "    \n",
    "    scraped_data.append({\n",
    "        \"Name\": name,\n",
    "        \"Born-Dead\": born_dead,\n",
    "        \"Term of Office\": term_of_office,\n",
    "        \"Remarks\": remarks\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae69462",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f26e33ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Motor1 webpage\n",
    "url = \"https://www.motor1.com/\"\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 2: Type in the search bar '50 most expensive cars' and click\n",
    "search_bar = soup.find(\"input\", id=\"search-input\")\n",
    "if search_bar:\n",
    "    search_bar[\"value\"] = \"50 most expensive cars\"\n",
    "    search_button = soup.find(\"button\", class_=\"header__search-button\")\n",
    "    if search_button:\n",
    "        search_button.click()\n",
    "        soup = BeautifulSoup(search_button.find_parent(\"form\").find_next_sibling().content, \"html.parser\")\n",
    "\n",
    "# Step 3: Click on \"50 most expensive cars in the world...\"\n",
    "expensive_cars_link = soup.find(\"a\", text=\"50 most expensive cars in the world...\")\n",
    "if expensive_cars_link:\n",
    "    expensive_cars_url = expensive_cars_link[\"href\"]\n",
    "    response = requests.get(expensive_cars_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Step 4: Scrape the data for the 50 most expensive cars\n",
    "car_listings = soup.find_all(\"div\", class_=\"text-list text-list--hover\")\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "for car in car_listings[:50]:\n",
    "    car_name = car.find(\"a\", class_=\"text-list__title\").text.strip()\n",
    "    car_price = car.find(\"span\", class_=\"text-list__subtitle\").text.strip()\n",
    "    \n",
    "    scraped_data.append({\n",
    "        \"Car Name\": car_name,\n",
    "        \"Price\": car_price\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf81c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
