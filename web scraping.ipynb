{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db39636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Header\n",
      "0                       Contents\n",
      "1                      Wikipedia\n",
      "2                        History\n",
      "3                        Nupedia\n",
      "4              Launch and growth\n",
      "..                           ...\n",
      "61              Academic studies\n",
      "62                         Books\n",
      "63  Book reviewâ€“related articles\n",
      "64          Other media coverage\n",
      "65                External links\n",
      "\n",
      "[66 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/Wikipedia'\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all header tags (h1 to h6)\n",
    "header_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Extract the text content of each header tag\n",
    "header_texts = [header.get_text() for header in header_tags]\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {'Header': header_texts}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85839d0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16704\\27960201.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Extract data from the table rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Skip the header row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the page\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "# Send a GET request to fetch the HTML content of the page\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the table containing the information\n",
    "table = soup.find('table', {'class': 'tablepress'})\n",
    "\n",
    "# Initialize lists to store president data\n",
    "names = []\n",
    "terms = []\n",
    "\n",
    "# Extract data from the table rows\n",
    "for row in table.find_all [1:]:  # Skip the header row\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) >= 2:\n",
    "        name = columns[0].get_text().strip()\n",
    "        term = columns[1].get_text().strip()\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "\n",
    "# Create a DataFrame from the extracted data\n",
    "data = {'Name': names, 'Term of Office': terms}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22c088b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "   Rank           Team Matches Points Rating\n",
      "0     1      Australia      23  2,714    118\n",
      "1     2       Pakistan      20  2,316    116\n",
      "2     3          India      36  4,081    113\n",
      "3     4    New Zealand      27  2,806    104\n",
      "4     5        England      24  2,426    101\n",
      "5     6   South Africa      19  1,910    101\n",
      "6     7     Bangladesh      28  2,661     95\n",
      "7     8    Afghanistan      16  1,404     88\n",
      "8     9      Sri Lanka      32  2,794     87\n",
      "9    10    West Indies      38  2,582     68\n",
      "10   11       Zimbabwe      30  1,641     55\n",
      "11   12       Scotland      33  1,662     50\n",
      "12   13        Ireland      24  1,052     44\n",
      "13   14    Netherlands      28  1,044     37\n",
      "14   15          Nepal      40  1,396     35\n",
      "15   16        Namibia      28    813     29\n",
      "16   17  United States      31    808     26\n",
      "17   18           Oman      24    525     22\n",
      "18   19            UAE      41    617     15\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16704\\260259668.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Scrape Top 10 ODI Batsmen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0modi_batsmen_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0modi_batsmen_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rankings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0modi_batsmen_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0modi_batsmen_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0modi_batsmen_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rank'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Batsman'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Team'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16704\\260259668.py\u001b[0m in \u001b[0;36mget_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mteam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'u-hide-phablet'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mpoints\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:  # Skipping header row\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].get_text().strip()\n",
    "        team = columns[1].find('span', {'class': 'u-hide-phablet'}).get_text().strip()\n",
    "        matches = columns[2].get_text().strip()\n",
    "        points = columns[3].get_text().strip()\n",
    "        rating = columns[4].get_text().strip()\n",
    "        data.append([rank, team, matches, points, rating])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Scrape Top 10 ODI teams\n",
    "odi_teams_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "odi_teams_data = get_rankings(odi_teams_url)\n",
    "odi_teams_df = pd.DataFrame(odi_teams_data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(odi_teams_df)\n",
    "\n",
    "# Scrape Top 10 ODI Batsmen\n",
    "odi_batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "odi_batsmen_data = get_rankings(odi_batsmen_url)\n",
    "odi_batsmen_df = pd.DataFrame(odi_batsmen_data, columns=['Rank', 'Batsman', 'Team', 'Rating'])\n",
    "\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(odi_batsmen_df)\n",
    "\n",
    "# Scrape Top 10 ODI Bowlers\n",
    "odi_bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "odi_bowlers_data = get_rankings(odi_bowlers_url)\n",
    "odi_bowlers_df = pd.DataFrame(odi_bowlers_data, columns=['Rank', 'Bowler', 'Team', 'Rating'])\n",
    "\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "513e020b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16704\\4070719854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Scrape Top 10 Women's ODI teams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0modi_women_teams_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0modi_women_teams_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rankings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0modi_women_teams_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0modi_women_teams_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0modi_women_teams_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rank'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Team'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Matches'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Points'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16704\\4070719854.py\u001b[0m in \u001b[0;36mget_rankings\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mrank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mplayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'u-hide-phablet'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mteam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'u-hide-phablet'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mrating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mteam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrating\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_rankings(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    data = []\n",
    "    for row in rows[1:]:  # Skipping header row\n",
    "        columns = row.find_all('td')\n",
    "        rank = columns[0].get_text().strip()\n",
    "        player = columns[1].find('span', {'class': 'u-hide-phablet'}).get_text().strip()\n",
    "        team = columns[2].find('span', {'class': 'u-hide-phablet'}).get_text().strip()\n",
    "        rating = columns[3].get_text().strip()\n",
    "        data.append([rank, player, team, rating])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Scrape Top 10 Women's ODI teams\n",
    "odi_women_teams_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "odi_women_teams_data = get_rankings(odi_women_teams_url)\n",
    "odi_women_teams_df = pd.DataFrame(odi_women_teams_data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "print(\"Top 10 Women's ODI Teams:\")\n",
    "print(odi_women_teams_df)\n",
    "\n",
    "# Scrape Top 10 Women's ODI Batting players\n",
    "odi_women_batting_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "odi_women_batting_data = get_rankings(odi_women_batting_url)\n",
    "odi_women_batting_df = pd.DataFrame(odi_women_batting_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "print(odi_women_batting_df)\n",
    "\n",
    "# Scrape Top 10 Women's ODI All-rounders\n",
    "odi_women_allrounders_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "odi_women_allrounders_data = get_rankings(odi_women_allrounders_url)\n",
    "odi_women_allrounders_df = pd.DataFrame(odi_women_allrounders_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "print(odi_women_allrounders_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aeee34bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16704\\3605949904.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Scrape news details\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mheadlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscrape_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Create a DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16704\\3605949904.py\u001b[0m in \u001b[0;36mscrape_news\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Card-time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mlink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Card-titleLink'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mfull_link\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"https://www.cnbc.com{link}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_news(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "\n",
    "    headlines = []\n",
    "    times = []\n",
    "    links = []\n",
    "\n",
    "    for article in articles:\n",
    "        headline = article.find('h3', class_='Card-title').text.strip()\n",
    "        time = article.find('time', class_='Card-time')['datetime']\n",
    "        link = article.find('a', class_='Card-titleLink')['href']\n",
    "        full_link = f\"https://www.cnbc.com{link}\"\n",
    "        \n",
    "        headlines.append(headline)\n",
    "        times.append(time)\n",
    "        links.append(full_link)\n",
    "    \n",
    "    return headlines, times, links\n",
    "\n",
    "# URL of the CNBC news page\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Scrape news details\n",
    "headlines, times, links = scrape_news(url)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Headline': headlines,\n",
    "    'Time': times,\n",
    "    'News Link': links\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a19890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('li', class_='result-list-item')\n",
    "\n",
    "    paper_titles = []\n",
    "    authors_list = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('h2', class_='title-text').text.strip()\n",
    "        authors = article.find('ul', class_='author-list').text.strip()\n",
    "        published_date = article.find('span', class_='published-online').text.strip()\n",
    "        link = article.find('a', class_='title-link')['href']\n",
    "        full_link = f\"https://www.journals.elsevier.com{link}\"\n",
    "        \n",
    "        paper_titles.append(title)\n",
    "        authors_list.append(authors)\n",
    "        published_dates.append(published_date)\n",
    "        paper_urls.append(full_link)\n",
    "    \n",
    "    return paper_titles, authors_list, published_dates, paper_urls\n",
    "\n",
    "# URL of the most downloaded articles page\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "# Scrape article details\n",
    "paper_titles, authors_list, published_dates, paper_urls = scrape_most_downloaded_articles(url)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Paper Title': paper_titles,\n",
    "    'Authors': authors_list,\n",
    "    'Published Date': published_dates,\n",
    "    'Paper URL': paper_urls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52d2329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('li', class_='result-list-item')\n",
    "\n",
    "    paper_titles = []\n",
    "    authors_list = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.find('h2', class_='title-text').text.strip()\n",
    "        authors = article.find('ul', class_='author-list').text.strip()\n",
    "        published_date = article.find('span', class_='published-online').text.strip()\n",
    "        link = article.find('a', class_='title-link')['href']\n",
    "        full_link = f\"https://www.journals.elsevier.com{link}\"\n",
    "        \n",
    "        paper_titles.append(title)\n",
    "        authors_list.append(authors)\n",
    "        published_dates.append(published_date)\n",
    "        paper_urls.append(full_link)\n",
    "    \n",
    "    return paper_titles, authors_list, published_dates, paper_urls\n",
    "\n",
    "# URL of the most downloaded articles page\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "# Scrape article details\n",
    "paper_titles, authors_list, published_dates, paper_urls = scrape_most_downloaded_articles(url)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Paper Title': paper_titles,\n",
    "    'Authors': authors_list,\n",
    "    'Published Date': published_dates,\n",
    "    'Paper URL': paper_urls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a4b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
